{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35cef746-8181-44ee-85be-5561b88035f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5842abbe-8f49-462c-b2e6-b14858a00c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafae66d-9912-4684-b279-3b714c88abfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x28a39d90d70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee349b9-9190-451a-b35f-952bdac1da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = nlp(\"John Adams is very good resercher than Vijay and Shardul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b562b9-e585-4e88-958e-b303a7c8018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John \t PROPN \t 11174346320140919546 \t John\n",
      "Adams \t PROPN \t 15278982565814068080 \t Adams\n",
      "is \t AUX \t 10382539506755952630 \t be\n",
      "very \t ADV \t 9548244504980166557 \t very\n",
      "good \t ADJ \t 5711639017775284443 \t good\n",
      "resercher \t NOUN \t 3520824235182541174 \t resercher\n",
      "than \t ADP \t 10794458019344880855 \t than\n",
      "Vijay \t PROPN \t 6324135320045123879 \t Vijay\n",
      "and \t CCONJ \t 2283656566040971221 \t and\n",
      "Shardul \t PROPN \t 11954645506361459269 \t Shardul\n"
     ]
    }
   ],
   "source": [
    "for token in var1:\n",
    "    print(token.text, '\\t' , token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af780d0-6187-490f-b48e-74997d9a36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391af0c5-1a76-4ff9-8aee-6d49797757d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4d2e27-272d-4c52-aefe-2ce2d0791893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'thence', 'whether', 'all', 'back', 'enough', 'down', 'they', 'will', 'itself', 'several', 'name', 'but', 'whole', 'sometimes', 'we', 'twenty', 'among', 'whereas', \"'ll\", 'beyond', 'to', 'otherwise', 'another', 'thereafter', 'more', 'nine', 'into', 'toward', 'becoming', 'were', 'anywhere', 'even', 'none', 'a', 'last', 'if', 'indeed', 'she', 'full', 'herein', 'just', 'take', 'really', 'such', 'while', 'became', 'once', 'behind', 'between', 'am', 'not', 'out', 'therefore', 'thereupon', 'put', 'latterly', 'because', 'the', \"'d\", '’m', 'nobody', 'through', 'unless', 'meanwhile', 'anything', 'where', 'five', 'over', 'wherein', '’ll', 'again', 'upon', 'sometime', 'former', 'elsewhere', 'anyway', '‘s', 'well', '‘d', 'please', 'within', 'noone', 'when', 'as', 'least', 'serious', 'side', 'without', 'many', 'by', 'seeming', 'could', 'most', 'does', 'hers', 'whenever', 'towards', 'beside', 'being', 'whereupon', 'seems', 'become', 'at', 'your', 'namely', 'much', 'fifteen', 'cannot', 'using', 'has', 'therein', 'until', 'whatever', 'give', 'across', 'thus', '’s', 'before', 'whence', 'nor', 'along', 'part', 'only', 'third', 'others', 'from', 'same', 'amount', 'often', 'first', 'next', 'during', 'besides', 'wherever', 'those', 'few', 'our', '‘ll', 'are', 'empty', 'my', 'be', 'how', 'ca', 'each', '’ve', 'beforehand', 'must', 'due', 'six', 'get', 'ten', 'throughout', 'mostly', 'hundred', 'latter', 'any', 'always', 'every', 'been', 'n‘t', 'four', 'hereupon', 'did', 'seem', 'per', 'almost', 'under', 'however', 'front', 'ours', 'seemed', 'hence', 'ever', 'everything', 'moreover', 'herself', 'yourselves', 'formerly', 'hereafter', 'then', 'now', 'very', 'whose', 'top', 'ourselves', 'becomes', 'although', '‘m', 'n’t', 'bottom', 'quite', 'of', 'something', 'or', 'there', 'so', 'around', 'an', 'own', 'yourself', 'do', 'and', 'thereby', \"'re\", '’d', 'amongst', 'might', 'also', 'else', 'rather', 'had', 'see', 'everywhere', 'you', 'already', 'eight', 'show', 'regarding', 'what', 'whither', 'whom', 'together', 'nothing', 'would', 'after', 'say', 'myself', 'fifty', 'used', 'their', 'her', 'who', 'three', 'him', 'whereafter', 'me', 'off', 'this', '’re', 'for', 'anyhow', 'whereby', 'doing', 'it', 'alone', 'though', 'why', 'done', 'too', 're', 'these', 'he', 'hereby', 'never', 'his', 'eleven', 'no', 'some', 'with', 'somewhere', 'keep', 'below', 'either', 'everyone', 'since', 'should', 'that', 'on', 'which', 'one', \"n't\", 'afterwards', 'here', '‘re', 'thru', 'was', 'further', 'forty', 'than', 'mine', 'i', 'move', 'yet', 'sixty', 'may', 'whoever', 'other', 'them', 'anyone', 'somehow', 'perhaps', 'nowhere', 'call', 'various', 'is', 'us', 'someone', 'still', 'twelve', 'about', 'onto', 'have', 'make', 'up', 'via', 'made', '‘ve', \"'s\", 'except', \"'m\", \"'ve\", 'neither', 'two', 'nevertheless', 'themselves', 'both', 'can', 'above', 'against', 'its', 'himself', 'in', 'go', 'less'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbb113e-605a-41e7-8f91-aec83ead0a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ab93dc-93b3-4591-8abc-7ace6bc1424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc7742c-4ac1-4f55-b078-a42da99efe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['mystery'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d317ef35-40d1-4fb7-940f-ca9a3894540b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7377c7-3cba-4c1c-bfb5-4c97b2502418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6e017b-242e-4699-a4ee-75114195ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Nimish Sharad Mothghare and good in Flutter Development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9c693d-4570-45c4-bffc-62bb01f641cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Nimish', 'Sharad', 'Mothghare', 'and', 'good', 'in', 'Flutter', 'Development']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ec1cd7-496e-448e-8e20-51d4871f5b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'nimish', 'sharad', 'mothghare', 'and', 'good', 'in', 'flutter', 'development']\n"
     ]
    }
   ],
   "source": [
    "tokens = [W.lower() for W in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9388941-c592-46fb-a742-9d109851be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina Tina PROPN NNP nsubj Xxxx True False\n",
      "purchase purchase VERB VBP ROOT xxxx True False\n",
      "the the DET DT det xxx True True\n",
      "Macbook Macbook PROPN NNP dobj Xxxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "purchase purchase NOUN NN nsubj xxxx True False\n",
      "also also ADV RB advmod xxxx True True\n",
      "accessories accessorie VERB VBZ conj xxxx True False\n",
      "regerading regerade VERB VBG amod xxxx True False\n",
      "macbook macbook NOUN NN dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "text = \"Tina purchase the Macbook and purchase also accessories regerading macbook\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05cb352-82b0-46e7-8fbe-46878288d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "['Prs']\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[0]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "print(token.morph.get(\"PronType\"))  # ['Prs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62213f09-3c76-4a9a-9643-d3a44ae349eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number=Plur\n",
      "NOUN\n",
      "Wo bist du?\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wo bist du?\") # English: 'Where are you?'\n",
    "print(doc[2].morph)  # 'Case=Nom|Number=Sing|Person=2|PronType=Prs'\n",
    "print(doc[2].pos_) # 'PRON'\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf3e524-5a15-4142-b17e-4a5d5b9ec0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert doc.has_annotation(\"SENT_START\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d88ac2c-e742-492d-bffa-ae4d7695c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnc_machine_details = nlp(\"The 7 major parts of a CNC machine typically include the Machine Control Unit (MCU). (the brain), the Drive System (motors/actuators for movement),. the Spindle (holds cutting tool), the Feedback System (sensors for precision), the Workholding/Clamping System, the Machine Assembly/Bed (frame/structure),  and the Input Device/Control Panel for programming and operatio \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaf99e12-9c24-40b1-b4e5-3a26ee39988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7 major parts of a CNC machine typically include the Machine Control Unit (MCU).\n",
      "(the brain), the Drive System (motors/actuators for movement),.\n",
      "the Spindle (holds cutting tool), the Feedback System (sensors for precision), the Workholding/Clamping System, the Machine Assembly/Bed (frame/structure),  and the Input Device/Control Panel for programming and operatio\n"
     ]
    }
   ],
   "source": [
    "assert cnc_machine_details.has_annotation(\"SENT_START\")\n",
    "for sent in cnc_machine_details.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a38eb105-fefa-4b59-8114-49abc5801a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3d97fa5-161b-46af-821e-281595f8494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3591f863-3752-47fe-aa2e-dbca82a97f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1145df83-580a-40af-a483-9e77baead695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94d58c9-eab6-43e1-b576-131c2fa723d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "nlp = English()  # just the language with no pipeline\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5572073e-b937-4814-8816-5cbe435f1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "913c7d53-b064-464d-80dc-ead4d407ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['this is a sentence...hello...and another sentence.']\n",
      "After: ['this is a sentence...', 'hello...', 'and another sentence.']\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a sentence...hello...and another sentence.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"Before:\", [sent.text for sent in doc.sents])\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \"...\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "doc = nlp(text)\n",
    "print(\"After:\", [sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36578945-05ac-4b7c-ab37-3a966192e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 6.1648946 True\n",
      "cat True 6.1186795 True\n",
      "banana True 6.7451453 True\n",
      "afskfsd True 6.307013 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6fe4726-f042-4950-8a65-fa39d7e5bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "019d2bdd-6bdc-4953-b1c5-54df478fa687",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I have send the resume so please check it and give me the positive response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b27bcbce-5204-4355-90ee-4f115d04eb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have': 3, 'send': 10, 'the': 12, 'resume': 9, 'so': 11, 'please': 6, 'check': 1, 'it': 4, 'and': 0, 'give': 2, 'me': 5, 'positive': 7, 'response': 8}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ea89898-bd5b-4cd3-bdcd-51d87086e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3694966d-9d2a-45c5-ab73-11030380c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5ccd51e-7975-4c3c-ab81-1bcb4d536ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "wast\n",
      "wait\n",
      "sung\n"
     ]
    }
   ],
   "source": [
    "e_words = [\"playing\",\"wasted\",\"waiting\",\"sung\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootwords = ps.stem(w)\n",
    "    print(rootwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c196879d-3c4b-4039-b3ed-c809a0f0b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "206eb09b-cb0b-4264-ac92-9c1a2038562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2017001b-2a27-466c-a2a6-1db266e0998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "985e4e6e-bf87-437d-85cd-ec88f167025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep\n",
      "time\n",
      "sing\n",
      "take\n"
     ]
    }
   ],
   "source": [
    "e_words = ['sleeping',\"timing\",\"sung\",\"taken\"]\n",
    "for w in e_words:\n",
    "    print(lemmatizer.lemmatize(w,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16738d3e-a855-48f1-9baa-bc70f622c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "remain\n",
      "render\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "f_words = [\"jumping\",\"remaining\",\"rendered\",\"ate\"]\n",
    "for w in f_words:\n",
    "    print(lemmatizer.lemmatize(w,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66cd55d0-7fe4-4e83-a8de-df5afda39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70dd08a8-aa39-4af4-886b-66084720dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stopwords are {\"she'd\", 'hasn', 'yours', \"they'd\", \"i'd\", 'did', 'when', 'as', \"haven't\", \"he's\", 'doing', 'needn', 'ma', 'under', 'all', 'it', \"she's\", \"they'll\", \"aren't\", 'ours', 'down', 'by', 'they', \"they've\", 'itself', \"he'd\", 'will', 'why', 'wouldn', 'most', 'too', 're', 'these', 'hadn', 'but', 'mustn', 'he', 'does', 'hers', 'we', \"you're\", 'being', 'herself', \"weren't\", 'his', 'no', 'some', 'weren', \"isn't\", 'with', 'yourselves', 'd', 'to', \"he'll\", 'at', 'below', 'having', 'your', \"i've\", 'then', 'now', 'more', \"didn't\", 'into', 'should', 'very', \"we're\", 'shouldn', 'ourselves', 'that', \"hasn't\", 'on', 'were', \"you'll\", 's', 'which', 'has', 'theirs', 'until', \"we've\", 'of', 'don', 'or', 't', 'before', 'here', 'nor', 'so', 'there', 'a', \"wouldn't\", 'if', 'she', 'doesn', 've', \"we'll\", 'an', 'only', 'own', 'won', 'was', 'further', \"you'd\", 'yourself', 'just', \"wasn't\", \"you've\", 'couldn', 'from', 'same', 'do', 'and', 'than', 'i', 'such', 'aren', \"couldn't\", 'had', \"hadn't\", \"she'll\", \"they're\", 'didn', 'while', 'during', 'once', 'those', 'other', 'you', \"should've\", 'few', 'our', 'them', 'between', \"shan't\", 'am', \"it'd\", 'not', 'are', 'out', \"doesn't\", 'my', 'be', 'how', 'is', 'isn', 'what', \"shouldn't\", 'because', \"i'll\", 'll', \"it's\", 'the', 'whom', 'wasn', 'each', 'm', 'y', 'about', \"that'll\", 'through', \"needn't\", 'after', 'have', 'myself', 'mightn', 'up', \"i'm\", 'where', 'over', 'haven', \"it'll\", \"won't\", 'their', 'her', \"mightn't\", 'ain', 'both', 'themselves', 'can', \"we'd\", 'above', 'any', 'who', 'again', 'against', 'its', \"mustn't\", 'him', 'himself', 'me', 'shan', 'in', 'off', 'this', \"don't\", 'o', 'been', 'for'}\n",
      "The words are ['AI', 'is', 'not', 'good', 'it', 'will', 'take', 'the', 'job', 'of', 'Developer']\n"
     ]
    }
   ],
   "source": [
    "data = \"AI is not good it will take the job of Developer\"\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(\"The stopwords are\",stopwords)\n",
    "words = word_tokenize(data)\n",
    "print(\"The words are\", words)\n",
    "wordfilterzed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a923a0a4-10b1-4693-8912-2d737ac980e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI', 'good', 'take', 'job', 'Developer']\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        wordfilterzed.append(w)\n",
    "\n",
    "print(wordfilterzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8b15c-3b86-4df0-8e65-cbf95660edd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
